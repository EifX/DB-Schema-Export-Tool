# Required: Directory to save the schema files
OutputDir=F:\Cached_DBSchema_SQLServer_to_PgSql

# Required: Database server name; assumed to be Microsoft SQL Server unless /PgUser is provided (or PgUser is defined in the .conf file)
Server=Proteinseqs

# Existing schema (DDL) file to parse to rename columns based on information in the ColumnMap file; will also skip any tables with <skip> in the DataTables file.
ExistingDDL=Manager_Control.sql

# With SQL Server databases, dump table data using COPY commands instead of INSERT INTO statements.  With PostgreSQL data, dump table data using pg_dump and COPY commands.
PgDump=True

# Number of values to insert at a time when PgInsert is true for a table; only applicable when exporting data from SQL Server
PgInsertChunkSize=50000

# With SQL Server databases, while reading the TableDataToExport.txt file, default the PgInsert column to true, meaning table data will be exported via INSERT INTO statements using the ON CONFLICT (key_column) DO UPDATE SET syntax. Ignored for PostgreSQL data
PgInsert=True

# Database name (alternatively, use DBList for a list of databases)
DB=Manager_Control

# Prefix name for output directories; default name is DBSchema__DatabaseName
DirectoryPrefix=DBSchema__

# Skip exporting schema
NoSchema=True

# Disable creating a subdirectory for each database when copying data to the Sync directory
NoSubdirectory=False

# Create a directory for the schema files for each database
CreateDBDirectories=True

# Export server settings, logins, and jobs
ServerInfo=False

# Text file with table names (one name per line) for which table data should be exported.
# Also supports a multi-column, tab-delimited format:
# SourceTableName TargetSchemaName TargetTableName PgInsert KeyColumn(s)
DataTables=Manager_Control_Tables.tsv

# Text file mapping source column names to target column names.
# Tab-delimited columns are:
# SourceTableName  SourceColumnName  TargetColumnName
# The TargetColumn supports <skip> for not including the given column in the output file
ColumnMap=Manager_Control_Table_Columns.tsv

# Table name (or comma separated list of names) to restrict
# table export operations. This is useful for exporting the
# data from just a single table
TableFilterList=

# Text file used to filter data by date when exporting data from tables.
# Tab-delimited columns are:
# SourceTableName  DateColumnName  MinimumDate
TableDataDateFilter=

# Default schema for exported tables and data. If undefined, use the original table's schema
DefaultSchema=mc

# In addition to table names defined by DataTables, there are default tables which will have their data exported.
# Disable the defaults using NoAutoData=True (or /NoAutoData)
NoAutoData=True

# Use NoTableData=True (or /NoTableData) to prevent any table data from being exported
# This parameter useful when processing an existing DDL file with ExistingDDL
NoTableData=True

# Export data from every table in the database; ignored if NoTableData is true
ExportAllData=False

# Maximum number of rows of data to export; defaults to 1000
# Use 0 to export all rows
MaxRows=100

# Generate a bash script for loading table data
ScriptLoad=False

# Auto changes column names from Upper_Case and UpperCase to lower_case when exporting table data
SnakeCase=True

# Copy new/changed files from the output directory to an alternative directory. This is advantageous to prevent file timestamps from getting updated every time the schema is exported.
Sync=

# Auto-update any new or changed files using Git
Git=False

# Commit any updates to the repository
Commit=False

# Log messages to a file; specify a log file name using /LogFile:LogFilePath
CreateLogFile=False

# Log file name
BaseLogFileName=

# Specify the directory to save the log file in; by default, the log file is created in the current working directory
LogDir=

# Count the number of database objects that would be exported
Preview=False

# Show (but do not log) export stats
Stats=False
